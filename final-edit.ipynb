{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install mediapipe","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-14T01:54:59.566012Z","iopub.execute_input":"2023-08-14T01:54:59.566597Z","iopub.status.idle":"2023-08-14T01:55:17.591551Z","shell.execute_reply.started":"2023-08-14T01:54:59.566544Z","shell.execute_reply":"2023-08-14T01:55:17.589807Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting mediapipe\n  Downloading mediapipe-0.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from mediapipe) (1.4.0)\nRequirement already satisfied: attrs>=19.1.0 in /opt/conda/lib/python3.10/site-packages (from mediapipe) (23.1.0)\nRequirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.10/site-packages (from mediapipe) (23.5.26)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from mediapipe) (3.7.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from mediapipe) (1.23.5)\nRequirement already satisfied: opencv-contrib-python in /opt/conda/lib/python3.10/site-packages (from mediapipe) (4.8.0.74)\nRequirement already satisfied: protobuf<4,>=3.11 in /opt/conda/lib/python3.10/site-packages (from mediapipe) (3.20.3)\nCollecting sounddevice>=0.4.4 (from mediapipe)\n  Downloading sounddevice-0.4.6-py3-none-any.whl (31 kB)\nRequirement already satisfied: CFFI>=1.0 in /opt/conda/lib/python3.10/site-packages (from sounddevice>=0.4.4->mediapipe) (1.15.1)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (1.1.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (4.40.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (1.4.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (2.8.2)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\nInstalling collected packages: sounddevice, mediapipe\nSuccessfully installed mediapipe-0.10.3 sounddevice-0.4.6\n","output_type":"stream"}]},{"cell_type":"markdown","source":"****Library Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport io\nimport shutil\nimport numpy as np\nimport pandas as pd\nimport pyarrow.parquet as pq\nimport tensorflow as tf\nimport json\nimport mediapipe\nimport imghdr\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport random\nfrom mediapipe.framework.formats import landmark_pb2\nfrom skimage.transform import resize\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tqdm.notebook import tqdm\nfrom PIL import Image","metadata":{"execution":{"iopub.status.busy":"2023-08-14T01:55:17.595034Z","iopub.execute_input":"2023-08-14T01:55:17.596221Z","iopub.status.idle":"2023-08-14T01:55:28.710272Z","shell.execute_reply.started":"2023-08-14T01:55:17.596168Z","shell.execute_reply":"2023-08-14T01:55:28.708969Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Create Mediapipe Hand Model ","metadata":{}},{"cell_type":"code","source":"# Extract the landmark data and convert it to an image using medipipe library.\n# This function extracts the data for both hands.\n#Ref: https://www.kaggle.com/code/gusthema/asl-fingerspelling-recognition-w-tensorflow\n\nmp_pose = mediapipe.solutions.pose\nmp_hands = mediapipe.solutions.hands\nmp_drawing = mediapipe.solutions.drawing_utils \nmp_drawing_styles = mediapipe.solutions.drawing_styles\n\ndef get_hands(seq_df):\n    images = []\n    all_hand_landmarks = []\n    for seq_idx in range(len(seq_df)):\n        x_hand = seq_df.iloc[seq_idx].filter(regex=\"x_right_hand.*\").values\n        y_hand = seq_df.iloc[seq_idx].filter(regex=\"y_right_hand.*\").values\n        z_hand = seq_df.iloc[seq_idx].filter(regex=\"z_right_hand.*\").values\n\n        right_hand_image = np.zeros((256, 256, 3))\n\n        right_hand_landmarks = landmark_pb2.NormalizedLandmarkList()\n        \n        for x, y, z in zip(x_hand, y_hand, z_hand):\n            right_hand_landmarks.landmark.add(x=x, y=y, z=z)\n\n        mp_drawing.draw_landmarks(\n                right_hand_image,\n                right_hand_landmarks,\n                mp_hands.HAND_CONNECTIONS,\n                landmark_drawing_spec=mp_drawing_styles.get_default_hand_landmarks_style())\n        \n        x_hand = seq_df.iloc[seq_idx].filter(regex=\"x_left_hand.*\").values\n        y_hand = seq_df.iloc[seq_idx].filter(regex=\"y_left_hand.*\").values\n        z_hand = seq_df.iloc[seq_idx].filter(regex=\"z_left_hand.*\").values\n        \n        left_hand_image = np.zeros((256, 256, 3))\n        \n        left_hand_landmarks = landmark_pb2.NormalizedLandmarkList()\n        for x, y, z in zip(x_hand, y_hand, z_hand):\n            left_hand_landmarks.landmark.add(x=x, y=y, z=z)\n\n        mp_drawing.draw_landmarks(\n                left_hand_image,\n                left_hand_landmarks,\n                mp_hands.HAND_CONNECTIONS,\n                landmark_drawing_spec=mp_drawing_styles.get_default_hand_landmarks_style())\n        \n        images.append([right_hand_image.astype(np.uint8), left_hand_image.astype(np.uint8)])\n        all_hand_landmarks.append([right_hand_landmarks, left_hand_landmarks])\n    return images, all_hand_landmarks","metadata":{"execution":{"iopub.status.busy":"2023-08-14T01:55:34.078786Z","iopub.execute_input":"2023-08-14T01:55:34.079181Z","iopub.status.idle":"2023-08-14T01:55:34.093510Z","shell.execute_reply.started":"2023-08-14T01:55:34.079151Z","shell.execute_reply":"2023-08-14T01:55:34.092552Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Loading and Preprocess\n","metadata":{}},{"cell_type":"code","source":"#Read train csv\ntrain_df = pd.read_csv('/kaggle/input/asl-fingerspelling/train.csv')\nno_phrases = train_df.shape[0]\nunique_file_ids = train_df['file_id'].unique()\nprint(len(unique_file_ids))\n\n# Get hand and pose columns\nLH = [f'x_left_hand_{i}' for i in range(21)] + [f'y_left_hand_{i}' for i in range(21)]\nRH = [f'x_right_hand_{i}' for i in range(21)] + [f'y_right_hand_{i}' for i in range(21)]\npose = [f'x_pose_{i}' for i in range(33)] + [f'y_pose_{i}' for i in range(33)]\nselect_features = LH + RH + pose\n\npadding_image = None\n# Read sequences \nfor file_id in tqdm(unique_file_ids): \n#     sequence_id, file_id, phrase = train_df.iloc[i][['sequence_id', 'file_id', 'phrase']]\n    file_rows = train_df[train_df['file_id'] == file_id]\n    sequence_id, phrase = file_rows.iloc[0][['sequence_id', 'phrase']]\n    # Read sequences for a given phrase\n    seq = pq.read_table(f\"/kaggle/input/asl-fingerspelling/train_landmarks/{str(file_id)}.parquet\",\n        filters=[[('sequence_id', '=', sequence_id)],]).to_pandas()\n    seq_frames =  seq[select_features]\n    # Chose hand image with min nan values \n    hand_images, hand_landmarks = get_hands(seq)\n    seq_len = len(hand_images)\n    \n    print(len(hand_images))\n    chosen_hand = None\n    chosen_images = []\n    chosen_landmarks_list = None\n    for i in range(len(hand_images)):\n        right_hand_landmarks = hand_landmarks[i][0]\n        left_hand_landmarks = hand_landmarks[i][1]\n        right_hand_image = hand_images[i][0]\n        left_hand_image = hand_images[i][1]\n\n        # Check NaN values for right hand\n        right_na_count = sum(np.isnan([lm.x for lm in right_hand_landmarks.landmark])) \\\n                         + sum(np.isnan([lm.y for lm in right_hand_landmarks.landmark])) \\\n                         + sum(np.isnan([lm.z for lm in right_hand_landmarks.landmark]))\n\n        # Check NaN values for left hand\n        left_na_count = sum(np.isnan([lm.x for lm in left_hand_landmarks.landmark])) \\\n                        + sum(np.isnan([lm.y for lm in left_hand_landmarks.landmark])) \\\n                        + sum(np.isnan([lm.z for lm in left_hand_landmarks.landmark]))\n        \n        # Choose the hand with fewer NaN values\n        if right_na_count < left_na_count:\n            min_na_count = right_na_count\n            chosen_hand = \"right\"\n            chosen_images.append(right_hand_image)\n        elif(left_na_count < right_na_count):\n            min_na_count = left_na_count\n            chosen_hand = \"left\"\n            chosen_images.append(left_hand_image)\n        else:\n            chosen_hand = \"right as both hands are Nan\"\n            chosen_images.append(right_hand_image)\n            padding_image = right_hand_image\n\n    \n    #Set Max Frame length\n    MAX_LEN = 413 # Max no of images in a sequences are 380\n    \n    # Pad chosen_images \n    if(len(chosen_images) < 413):\n        itr = MAX_LEN-len(chosen_images)\n        for i in range(itr):\n            chosen_images.append(padding_image)\n    \n    print('Adjusted chosen image length is:',len(chosen_images))\n    \n    if not os.path.isdir(\"tfrecords\"): os.mkdir(\"tfrecords\")\n    tf_record = f\"tfrecords/{file_id}.tfrecord\"\n    \n    \n    # Create TF Records\n    parquet_numpy = seq.to_numpy()\n    with tf.io.TFRecordWriter(tf_record) as file_writer:\n        for i in range(len(chosen_images)):\n            image = chosen_images[i]\n            \n            # Preprocess and convert image to a supported format\n            preprocessed_image = image.astype(np.uint8)\n            \n            # Serialize the image array to bytes\n            image_bytes = tf.io.encode_jpeg(preprocessed_image).numpy()\n\n            # Create a TF feature for the image and phrase\n            feature = {\n            \"image\": tf.train.Feature(\n                bytes_list=tf.train.BytesList(value=[image_bytes])\n            ),\n            \"phrase\": tf.train.Feature(\n                bytes_list=tf.train.BytesList(value=[bytes(phrase, 'utf-8')])\n            ),\n        }\n\n            record_bytes = tf.train.Example(features=tf.train.Features(feature=feature)).SerializeToString()\n            file_writer.write(record_bytes)\n","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2023-08-14T01:55:47.943967Z","iopub.execute_input":"2023-08-14T01:55:47.944419Z","iopub.status.idle":"2023-08-14T02:04:18.093927Z","shell.execute_reply.started":"2023-08-14T01:55:47.944385Z","shell.execute_reply":"2023-08-14T02:04:18.091873Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"68\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/68 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f4e7e8266b94e5f999c10262fe6b2f6"}},"metadata":{}},{"name":"stdout","text":"123\nAdjusted chosen image length is: 413\n294\nAdjusted chosen image length is: 413\n127\nAdjusted chosen image length is: 413\n126\nAdjusted chosen image length is: 413\n122\nAdjusted chosen image length is: 413\n300\nAdjusted chosen image length is: 413\n132\nAdjusted chosen image length is: 413\n94\nAdjusted chosen image length is: 413\n107\nAdjusted chosen image length is: 413\n186\nAdjusted chosen image length is: 413\n209\nAdjusted chosen image length is: 413\n118\nAdjusted chosen image length is: 413\n61\nAdjusted chosen image length is: 413\n103\nAdjusted chosen image length is: 413\n8\nAdjusted chosen image length is: 413\n173\nAdjusted chosen image length is: 413\n60\nAdjusted chosen image length is: 413\n204\nAdjusted chosen image length is: 413\n150\nAdjusted chosen image length is: 413\n140\nAdjusted chosen image length is: 413\n244\nAdjusted chosen image length is: 413\n84\nAdjusted chosen image length is: 413\n77\nAdjusted chosen image length is: 413\n6\nAdjusted chosen image length is: 413\n293\nAdjusted chosen image length is: 413\n115\nAdjusted chosen image length is: 413\n291\nAdjusted chosen image length is: 413\n140\nAdjusted chosen image length is: 413\n170\nAdjusted chosen image length is: 413\n177\nAdjusted chosen image length is: 413\n184\nAdjusted chosen image length is: 413\n127\nAdjusted chosen image length is: 413\n106\nAdjusted chosen image length is: 413\n210\nAdjusted chosen image length is: 413\n145\nAdjusted chosen image length is: 413\n169\nAdjusted chosen image length is: 413\n139\nAdjusted chosen image length is: 413\n99\nAdjusted chosen image length is: 413\n380\nAdjusted chosen image length is: 413\n124\nAdjusted chosen image length is: 413\n3\nAdjusted chosen image length is: 413\n122\nAdjusted chosen image length is: 413\n141\nAdjusted chosen image length is: 413\n262\nAdjusted chosen image length is: 413\n121\nAdjusted chosen image length is: 413\n76\nAdjusted chosen image length is: 413\n156\nAdjusted chosen image length is: 413\n119\nAdjusted chosen image length is: 413\n95\nAdjusted chosen image length is: 413\n128\nAdjusted chosen image length is: 413\n238\nAdjusted chosen image length is: 413\n34\nAdjusted chosen image length is: 413\n122\nAdjusted chosen image length is: 413\n150\nAdjusted chosen image length is: 413\n311\nAdjusted chosen image length is: 413\n413\nAdjusted chosen image length is: 413\n133\nAdjusted chosen image length is: 413\n97\nAdjusted chosen image length is: 413\n228\nAdjusted chosen image length is: 413\n120\nAdjusted chosen image length is: 413\n128\nAdjusted chosen image length is: 413\n231\nAdjusted chosen image length is: 413\n148\nAdjusted chosen image length is: 413\n270\nAdjusted chosen image length is: 413\n69\nAdjusted chosen image length is: 413\n32\nAdjusted chosen image length is: 413\n219\nAdjusted chosen image length is: 413\n54\nAdjusted chosen image length is: 413\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Read TF Records","metadata":{}},{"cell_type":"code","source":"def decode_fn(record_bytes):\n    schema = {\n        \"image\": tf.io.FixedLenFeature([], dtype=tf.string),\n        \"phrase\": tf.io.FixedLenFeature([], dtype=tf.string)\n    }\n    features = tf.io.parse_single_example(record_bytes, schema)\n    \n    # Decode image\n#     image = tf.io.decode_raw(features['image'], tf.uint8)\n#     image = tf.reshape(image, (256, 256, 3))\n    image = tf.io.decode_jpeg(features['image'], channels=3)\n    image = tf.cast(image, tf.float32) / 255.0\n    \n    # Decode phrase\n#     phrase = tf.io.decode_raw(features['phrase'], tf.uint8)\n#     phrase = tf.strings.join([phrase], separator=\"\").numpy().decode('utf-8')\n    phrase = features['phrase']\n    \n    return image, phrase","metadata":{"execution":{"iopub.status.busy":"2023-08-14T02:04:22.836027Z","iopub.execute_input":"2023-08-14T02:04:22.836467Z","iopub.status.idle":"2023-08-14T02:04:22.844557Z","shell.execute_reply.started":"2023-08-14T02:04:22.836426Z","shell.execute_reply":"2023-08-14T02:04:22.843138Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Preprocess input sequences","metadata":{}},{"cell_type":"code","source":"#Ref: https://www.kaggle.com/code/gusthema/asl-fingerspelling-recognition-w-tensorflow\nwith open (\"/kaggle/input/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n    char_to_num = json.load(f)\n\n# Add pad_token, start_token and end_token to the dict\npad_token = 'P'\nstart_token = '<'\nend_token = '>'\npad_token_idx = 59\nstart_token_idx = 60\nend_token_idx = 61\n\nchar_to_num[pad_token] = pad_token_idx\nchar_to_num[start_token] = start_token_idx\nchar_to_num[end_token] = end_token_idx\nnum_to_char = {j:i for i,j in char_to_num.items()}\n\ntable = tf.lookup.StaticHashTable(\n    initializer=tf.lookup.KeyValueTensorInitializer(\n        keys=list(char_to_num.keys()),\n        values=list(char_to_num.values()),\n    ),\n    default_value=tf.constant(-1),\n    name=\"class_weight\"\n)\n\ndef convert_fn(landmarks, phrase):\n    # Add start and end pointers to phrase.\n    phrase = start_token + phrase + end_token\n    phrase = tf.strings.bytes_split(phrase)\n    phrase = table.lookup(phrase)\n    # Vectorize and add padding.\n    phrase = tf.pad(phrase, paddings=[[0, 64 - tf.shape(phrase)[0]]], mode = 'CONSTANT',\n                    constant_values = pad_token_idx)\n    return landmarks, phrase","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-08-14T02:16:32.847892Z","iopub.execute_input":"2023-08-14T02:16:32.848465Z","iopub.status.idle":"2023-08-14T02:16:32.869095Z","shell.execute_reply.started":"2023-08-14T02:16:32.848416Z","shell.execute_reply":"2023-08-14T02:16:32.867603Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"Split Data","metadata":{}},{"cell_type":"code","source":"#Get TF Records\ntf_records = train_df.file_id.map(lambda x: f'/kaggle/working/tfrecords/{x}.tfrecord').unique()\nprint(f\"List of {len(tf_records)} TFRecord files.\")\n\nbatch_size = 16\ntrain_len = int(0.1 * len(tf_records))\n\ntrain_ds = tf.data.TFRecordDataset(tf_records[:train_len]).map(decode_fn).map(convert_fn).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE).cache()\nvalid_ds = tf.data.TFRecordDataset(tf_records[train_len:]).map(decode_fn).map(convert_fn).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE).cache()\n\n# Verify TF Records being read properly\n\n# for landmarks, phrase in train_ds:\n#         # Convert the TensorFlow tensor to a NumPy array\n#     batch_of_images_np = landmarks.numpy()\n\n#     # Determine the shape of the batch (assuming the first dimension is the batch size)\n#     batch_size, height, width, channels = batch_of_images_np.shape\n\n#     # Calculate the number of rows and columns for the grid\n#     num_rows = int(np.sqrt(batch_size))\n#     num_cols = (batch_size + num_rows - 1) // num_rows\n\n#     # Create a figure and a grid of subplots\n#     fig, axs = plt.subplots(num_rows, num_cols, figsize=(10, 10))\n\n#     # Flatten the subplots array if it's a 1D array\n#     if num_rows == 1:\n#         axs = axs.reshape(1, -1)\n\n#     # Loop through the images and display them in the grid\n#     for i in range(batch_size):\n#         ax = axs[i // num_cols, i % num_cols]\n#         ax.imshow(batch_of_images_np[i])\n#         ax.axis('off')  # Turn off axis labels and ticks\n\n#     # Hide any remaining empty subplots\n#     for i in range(batch_size, num_rows * num_cols):\n#         axs[i // num_cols, i % num_cols].axis('off')\n\n#     # Show the grid of images\n#     plt.show()\n    \n#     print(\"Phrase:\", phrase)\n#     print(\"------------------------------\")","metadata":{"execution":{"iopub.status.busy":"2023-08-14T02:33:13.402730Z","iopub.execute_input":"2023-08-14T02:33:13.403275Z","iopub.status.idle":"2023-08-14T02:33:13.680060Z","shell.execute_reply.started":"2023-08-14T02:33:13.403235Z","shell.execute_reply":"2023-08-14T02:33:13.678754Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"List of 68 TFRecord files.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Transformer Model in progress.....","metadata":{}},{"cell_type":"code","source":"# Token And Landmark Image Embedding\nclass TokenEmbedding(layers.Layer):\n    def __init__(self, num_vocab=1000, maxlen=100, num_hid=64):\n        super().__init__()\n        self.emb = tf.keras.layers.Embedding(num_vocab, num_hid)\n        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n\n    def call(self, x):\n        maxlen = tf.shape(x)[-1]\n        x = self.emb(x)\n        positions = tf.range(start=0, limit=maxlen, delta=1)\n        positions = self.pos_emb(positions)\n        return x + positions\n\nclass LandmarkEmbedding2D(layers.Layer):\n    def __init__(self, num_hid=64, maxlen=100):\n        super().__init__()\n        self.conv1 = tf.keras.layers.Conv2D(\n            num_hid, (3, 3), strides=(2, 2), padding=\"same\", activation=\"relu\"\n        )\n        self.conv2 = tf.keras.layers.Conv2D(\n            num_hid, (3, 3), strides=(2, 2), padding=\"same\", activation=\"relu\"\n        )\n        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n\n    def call(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = tf.keras.layers.Flatten()(x)\n        # Apply the positional embedding\n        x = self.pos_emb(x)\n        # Expand the dimensions to match the desired output shape\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2023-08-14T01:47:33.078928Z","iopub.execute_input":"2023-08-14T01:47:33.080136Z","iopub.status.idle":"2023-08-14T01:47:33.094160Z","shell.execute_reply.started":"2023-08-14T01:47:33.080088Z","shell.execute_reply":"2023-08-14T01:47:33.092871Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Transformer Encoder","metadata":{}},{"cell_type":"code","source":"class TransformerEncoder(layers.Layer):\n    def __init__(self, embed_dim, num_heads, feed_forward_dim, rate=0.1):\n        super().__init__()\n        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = keras.Sequential(\n            [\n                layers.Dense(feed_forward_dim, activation=\"relu\"),\n                layers.Dense(embed_dim),\n            ]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)","metadata":{"execution":{"iopub.status.busy":"2023-08-14T01:47:33.096193Z","iopub.execute_input":"2023-08-14T01:47:33.096681Z","iopub.status.idle":"2023-08-14T01:47:33.112068Z","shell.execute_reply.started":"2023-08-14T01:47:33.096635Z","shell.execute_reply":"2023-08-14T01:47:33.110578Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Transformer Decoder","metadata":{}},{"cell_type":"code","source":"# Customized to add `training` variable\n# Reference: https://www.kaggle.com/code/shlomoron/aslfr-a-simple-transformer/notebook\n\nclass TransformerDecoder(layers.Layer):\n    def __init__(self, embed_dim, num_heads, feed_forward_dim, dropout_rate=0.1):\n        super().__init__()\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n        self.self_att = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim\n        )\n        self.enc_att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.self_dropout = layers.Dropout(0.5)\n        self.enc_dropout = layers.Dropout(0.1)\n        self.ffn_dropout = layers.Dropout(0.1)\n        self.ffn = keras.Sequential(\n            [\n                layers.Dense(feed_forward_dim, activation=\"relu\"),\n                layers.Dense(embed_dim),\n            ]\n        )\n\n    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n        \"\"\"Masks the upper half of the dot product matrix in self attention.\n\n        This prevents flow of information from future tokens to current token.\n        1's in the lower triangle, counting from the lower right corner.\n        \"\"\"\n        i = tf.range(n_dest)[:, None]\n        j = tf.range(n_src)\n        m = i >= j - n_src + n_dest\n        mask = tf.cast(m, dtype)\n        mask = tf.reshape(mask, [1, n_dest, n_src])\n        mult = tf.concat(\n            [batch_size[..., tf.newaxis], tf.constant([1, 1], dtype=tf.int32)], 0\n        )\n        return tf.tile(mask, mult)\n\n    def call(self, enc_out, target, training):\n        input_shape = tf.shape(target)\n        batch_size = input_shape[0]\n        seq_len = input_shape[1]\n        causal_mask = self.causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n        target_att = self.self_att(target, target, attention_mask=causal_mask)\n        target_norm = self.layernorm1(target + self.self_dropout(target_att, training = training))\n        enc_out = self.enc_att(target_norm, enc_out)\n        enc_out_norm = self.layernorm2(self.enc_dropout(enc_out, training = training) + target_norm)\n        ffn_out = self.ffn(enc_out_norm)\n        ffn_out_norm = self.layernorm3(enc_out_norm + self.ffn_dropout(ffn_out, training = training))\n        return ffn_out_norm","metadata":{"execution":{"iopub.status.busy":"2023-08-14T01:47:33.113869Z","iopub.execute_input":"2023-08-14T01:47:33.114520Z","iopub.status.idle":"2023-08-14T01:47:33.131304Z","shell.execute_reply.started":"2023-08-14T01:47:33.114487Z","shell.execute_reply":"2023-08-14T01:47:33.130103Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"Transformer Model","metadata":{}},{"cell_type":"code","source":"class Transformer(keras.Model):\n    def __init__(\n        self,\n        num_hid=64,\n        num_head=2,\n        num_feed_forward=128,\n        source_maxlen=100,\n        target_maxlen=100,\n        num_layers_enc=4,\n        num_layers_dec=1,\n        num_classes=60,\n    ):\n        super().__init__()\n        self.loss_metric = keras.metrics.Mean(name=\"loss\")\n        self.acc_metric = keras.metrics.Mean(name=\"edit_dist\")\n        self.num_layers_enc = num_layers_enc\n        self.num_layers_dec = num_layers_dec\n        self.target_maxlen = target_maxlen\n        self.num_classes = num_classes\n\n        self.enc_input = LandmarkEmbedding2D(num_hid=num_hid, maxlen=source_maxlen)\n        self.dec_input = TokenEmbedding(\n            num_vocab=num_classes, maxlen=target_maxlen, num_hid=num_hid\n        )\n\n        self.encoder = keras.Sequential(\n            [self.enc_input]\n            + [\n                TransformerEncoder(num_hid, num_head, num_feed_forward)\n                for _ in range(num_layers_enc)\n            ]\n        )\n\n        for i in range(num_layers_dec):\n            setattr(\n                self,\n                f\"dec_layer_{i}\",\n                TransformerDecoder(num_hid, num_head, num_feed_forward),\n            )\n\n        self.classifier = layers.Dense(num_classes)\n\n    def decode(self, enc_out, target, training):\n        y = self.dec_input(target)\n        for i in range(self.num_layers_dec):\n            y = getattr(self, f\"dec_layer_{i}\")(enc_out, y, training)\n        return y\n\n    def call(self, inputs, training):\n        source = inputs[0]\n        target = inputs[1]\n        x = self.encoder(source, training)\n        y = self.decode(x, target, training)\n        return self.classifier(y)\n\n    @property\n    def metrics(self):\n        return [self.loss_metric]\n\n    def train_step(self, batch):\n        \"\"\"Processes one batch inside model.fit().\"\"\"\n        source = batch[0]\n        target = batch[1]\n\n        input_shape = tf.shape(target)\n        batch_size = input_shape[0]\n        \n        dec_input = target[:, :-1]\n        dec_target = target[:, 1:]\n        with tf.GradientTape() as tape:\n            preds = self([source, dec_input])\n            one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n            mask = tf.math.logical_not(tf.math.equal(dec_target, pad_token_idx))\n            loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n        trainable_vars = self.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n        # Computes the Levenshtein distance between sequences since the evaluation\n        # metric for this contest is the normalized total levenshtein distance.\n        edit_dist = tf.edit_distance(tf.sparse.from_dense(target), \n                                     tf.sparse.from_dense(tf.cast(tf.argmax(preds, axis=1), tf.int32)))\n        edit_dist = tf.reduce_mean(edit_dist)\n        self.acc_metric.update_state(edit_dist)\n        self.loss_metric.update_state(loss)\n        return {\"loss\": self.loss_metric.result(), \"edit_dist\": self.acc_metric.result()}\n\n    def test_step(self, batch):        \n        source = batch[0]\n        target = batch[1]\n\n        input_shape = tf.shape(target)\n        batch_size = input_shape[0]\n        \n        dec_input = target[:, :-1]\n        dec_target = target[:, 1:]\n        preds = self([source, dec_input])\n        one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n        mask = tf.math.logical_not(tf.math.equal(dec_target, pad_token_idx))\n        loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n        # Computes the Levenshtein distance between sequences since the evaluation\n        # metric for this contest is the normalized total levenshtein distance.\n        edit_dist = tf.edit_distance(tf.sparse.from_dense(target), \n                                     tf.sparse.from_dense(tf.cast(tf.argmax(preds, axis=1), tf.int32)))\n        edit_dist = tf.reduce_mean(edit_dist)\n        self.acc_metric.update_state(edit_dist)\n        self.loss_metric.update_state(loss)\n        return {\"loss\": self.loss_metric.result(), \"edit_dist\": self.acc_metric.result()}\n\n    def generate(self, source, target_start_token_idx):\n        \"\"\"Performs inference over one batch of inputs using greedy decoding.\"\"\"\n        bs = tf.shape(source)[0]\n        enc = self.encoder(source, training = False)\n        dec_input = tf.ones((bs, 1), dtype=tf.int32) * target_start_token_idx\n        dec_logits = []\n        for i in range(self.target_maxlen - 1):\n            dec_out = self.decode(enc, dec_input, training = False)\n            logits = self.classifier(dec_out)\n            logits = tf.argmax(logits, axis=-1, output_type=tf.int32)\n            last_logit = logits[:, -1][..., tf.newaxis]\n            dec_logits.append(last_logit)\n            dec_input = tf.concat([dec_input, last_logit], axis=-1)\n        return dec_input","metadata":{"execution":{"iopub.status.busy":"2023-08-14T01:47:33.133660Z","iopub.execute_input":"2023-08-14T01:47:33.134170Z","iopub.status.idle":"2023-08-14T01:47:33.166933Z","shell.execute_reply.started":"2023-08-14T01:47:33.134125Z","shell.execute_reply":"2023-08-14T01:47:33.165568Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Display Output","metadata":{}},{"cell_type":"code","source":"class DisplayOutputs(keras.callbacks.Callback):\n    def __init__(\n        self, batch, idx_to_token, target_start_token_idx=60, target_end_token_idx=61\n    ):\n        \"\"\"Displays a batch of outputs after every 4 epoch\n\n        Args:\n            batch: A test batch\n            idx_to_token: A List containing the vocabulary tokens corresponding to their indices\n            target_start_token_idx: A start token index in the target vocabulary\n            target_end_token_idx: An end token index in the target vocabulary\n        \"\"\"\n        self.batch = batch\n        self.target_start_token_idx = target_start_token_idx\n        self.target_end_token_idx = target_end_token_idx\n        self.idx_to_char = idx_to_token\n\n    def on_epoch_end(self, epoch, logs=None):\n        if epoch % 4 != 0:\n            return\n        source = self.batch[0]\n        target = self.batch[1].numpy()\n        bs = tf.shape(source)[0]\n        preds = self.model.generate(source, self.target_start_token_idx)\n        preds = preds.numpy()\n        for i in range(bs):\n            target_text = \"\".join([self.idx_to_char[_] for _ in target[i, :]])\n            prediction = \"\"\n            for idx in preds[i, :]:\n                prediction += self.idx_to_char[idx]\n                if idx == self.target_end_token_idx:\n                    break\n            print(f\"target:     {target_text.replace('-','')}\")\n            print(f\"prediction: {prediction}\\n\")\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-08-14T01:47:33.168826Z","iopub.execute_input":"2023-08-14T01:47:33.169351Z","iopub.status.idle":"2023-08-14T01:47:33.185381Z","shell.execute_reply.started":"2023-08-14T01:47:33.169282Z","shell.execute_reply":"2023-08-14T01:47:33.184074Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Train transformer ","metadata":{}},{"cell_type":"code","source":"batch = next(iter(valid_ds))\n\n# The vocabulary to convert predicted indices into characters\nidx_to_char = list(char_to_num.keys())\ndisplay_cb = DisplayOutputs(\n    batch, idx_to_char, target_start_token_idx=char_to_num['<'], target_end_token_idx=char_to_num['>']\n)  # set the arguments as per vocabulary index for '<' and '>'\n\nmodel = Transformer(\n    num_hid=200,\n    num_head=4,\n    num_feed_forward=400,\n    source_maxlen = 413,\n    target_maxlen=64,\n    num_layers_enc=2,\n    num_layers_dec=1,\n    num_classes=62\n)\nloss_fn = tf.keras.losses.CategoricalCrossentropy(\n    from_logits=True, label_smoothing=0.1,\n)\n\n\noptimizer = keras.optimizers.Adam(0.0001)\nmodel.compile(optimizer=optimizer, loss=loss_fn)\n\nhistory = model.fit(train_ds, validation_data=valid_ds, callbacks=[display_cb], epochs=13)","metadata":{"execution":{"iopub.status.busy":"2023-08-14T01:54:39.028165Z","iopub.execute_input":"2023-08-14T01:54:39.028652Z","iopub.status.idle":"2023-08-14T01:54:39.565806Z","shell.execute_reply.started":"2023-08-14T01:54:39.028607Z","shell.execute_reply":"2023-08-14T01:54:39.563900Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[43mvalid_ds\u001b[49m))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# The vocabulary to convert predicted indices into characters\u001b[39;00m\n\u001b[1;32m      4\u001b[0m idx_to_char \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(char_to_num\u001b[38;5;241m.\u001b[39mkeys())\n","\u001b[0;31mNameError\u001b[0m: name 'valid_ds' is not defined"],"ename":"NameError","evalue":"name 'valid_ds' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"Create TFLite Model and Prepare for Submission","metadata":{}},{"cell_type":"code","source":" class TFLiteModel(tf.Module):\n    def __init__(self, model):\n        super(TFLiteModel, self).__init__()\n        self.target_start_token_idx = start_token_idx\n        self.target_end_token_idx = end_token_idx\n        # Load the feature generation and main models\n        self.model = model\n    \n    @tf.function(input_signature=[tf.TensorSpec(shape=[None, len(FEATURE_COLUMNS)], dtype=tf.float32, name='inputs')])\n    def __call__(self, inputs, training=False):\n        # Preprocess Data\n        x = tf.cast(inputs, tf.float32)\n        x = x[None]\n        x = tf.cond(tf.shape(x)[1] == 0, lambda: tf.zeros((1, 1, len(FEATURE_COLUMNS))), lambda: tf.identity(x))\n        x = x[0]\n        x = pre_process(x)\n        x = x[None]\n        x = self.model.generate(x, self.target_start_token_idx)\n        x = x[0]\n        idx = tf.argmax(tf.cast(tf.equal(x, self.target_end_token_idx), tf.int32))\n        idx = tf.where(tf.math.less(idx, 1), tf.constant(2, dtype=tf.int64), idx)\n        x = x[1:idx]\n        x = tf.one_hot(x, 59)\n        return {'outputs': x}\n    \ntflitemodel_base = TFLiteModel(model)\nmodel.save_weights(\"model.h5\")\n\nkeras_model_converter = tf.lite.TFLiteConverter.from_keras_model(tflitemodel_base)\nkeras_model_converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]#, tf.lite.OpsSet.SELECT_TF_OPS]\ntflite_model = keras_model_converter.convert()\nwith open('/kaggle/working/model.tflite', 'wb') as f:\n    f.write(tflite_model)\n    \ninfargs = {\"selected_columns\" : FEATURE_COLUMNS}\n\nwith open('inference_args.json', \"w\") as json_file:\n    json.dump(infargs, json_file)\n\n!zip submission.zip  './model.tflite' './inference_args.json'\n\ninterpreter = tf.lite.Interpreter(\"model.tflite\")\n\nREQUIRED_SIGNATURE = \"serving_default\"\nREQUIRED_OUTPUT = \"outputs\"\n\nwith open (\"/kaggle/input/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n    character_map = json.load(f)\nrev_character_map = {j:i for i,j in character_map.items()}\n\nfound_signatures = list(interpreter.get_signature_list().keys())\n\nif REQUIRED_SIGNATURE not in found_signatures:\n    raise KernelEvalException('Required input signature not found.')\n\nprediction_fn = interpreter.get_signature_runner(\"serving_default\")\noutput = prediction_fn(inputs=batch[0][0])\nprediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output[REQUIRED_OUTPUT], axis=1)])\nprint(prediction_str)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}